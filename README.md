# Sign-Language-Translator

## Project Idea
End Goal is to develop an in-house device that can do the following:

**Audio-Translation:** The audio from a person is taken as an input and an appropriate gesture (or combination of gestures) is output on the screen (we will have to create standard templates for the gestures) in real-time.

**Gesture-Translation:** The images captured from the gestures of a person are processed in real-time and output the audio (display the text on the screen) corresponding to the gesture (or combination of gestures)




## Learning Outcomes
Practical Project Experience in Computer Vision Using Tensorflow and/or PyTorch Libraries
 
 
## Plan of Action or Time Line

The main aspects of this project would be to deal right with the basic of identifying thr  relevant datasets and papers ans directly the final steps of deployment odf theis algorithms 

1) Identify the relevant datasets and papers on this domain
2) Pick up a model/ Create a model that works
3) Experimentation : Perform various experiments on different datasets, different parameters and note the observation
4) Improve the model to increase it's accurancy and/or reduce processing time so that it can be fitted into a device with low processing power





## Pre-req
1) Should have a basic knowledge of Python and OOPs in python
2) Familiarity with using Git 
3) Basic theoritical concepts of Computer Vision 
4) Passion for  Learning

## End- Result


## Expectations
You should be able to write clean efficient code with proper commenting and documentations of each experiment


## Project Mentors
Viraj V Aute

Hardik Shah
